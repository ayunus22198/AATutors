{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "Copy of wavenet_vocoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "rise": {
      "scroll": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayunus22198/AATutors/blob/master/Copy_of_wavenet_vocoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnyMjCRLOCGD",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/wavenet_vocoder.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaEZSSDiOJ0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIdNzw-AOCGE",
        "colab_type": "text"
      },
      "source": [
        "# WaveNet Vocoder Recipe Demonstration\n",
        "\n",
        "**Tomoki Hayashi**\n",
        "\n",
        "Department of Informatics, Nagoya University  \n",
        "Human Dataware Lab. Co., Ltd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkd2X4jVOCGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsn_L5mqOCGI",
        "colab_type": "text"
      },
      "source": [
        "## Environmental setup\n",
        "\n",
        "First, install dependecies (It takes several minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "scrolled": true,
        "id": "u8SOAeSoOCGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -qq -y bc tree\n",
        "!git clone https://github.com/kan-bayashi/PytorchWaveNetVocoder.git -b IS19TUTORIAL\n",
        "!git clone https://github.com/k2kobayashi/sprocket.git -b IS19TUTORIAL\n",
        "!cd sprocket && pip install -q .\n",
        "!cd PytorchWaveNetVocoder && pip install -q .\n",
        "!cd PytorchWaveNetVocoder && mkdir -p tools/venv/bin && touch tools/venv/bin/activate\n",
        "import sprocket, wavenet_vocoder  # check importable\n",
        "!echo \"Setup done!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "908ABaIQOCGL",
        "colab_type": "text"
      },
      "source": [
        "## What is the PytorchWaveNetVocoder?\n",
        "\n",
        "Github: [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder)  \n",
        "Samples: https://kan-bayashi.github.io/WaveNetVocoderSamples/\n",
        "\n",
        "- WaveNet vocoder implemention with pytorch\n",
        "- Support [kaldi](https://github.com/kaldi-asr/kaldi)-like recipes, easy to reproduce the results\n",
        "- Support [World](https://github.com/mmorise/World) features / mel-spectrogram based models\n",
        "- Support multi-gpu training / decoding\n",
        "- Support a noise shaping [[Tachibana+ 2018](https://ieeexplore.ieee.org/document/8461332)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLpxuWABOCGL",
        "colab_type": "text"
      },
      "source": [
        "## What it the kaldi-like recipe?\n",
        "\n",
        "Key features:\n",
        "- Prepared for each corpus (e.g. CMU Arctic, LJSpeech)\n",
        "- Consists of unified several stages  \n",
        "  (e.g. data preparation, feature extraction, and so on.)\n",
        "- Includes all procedures needed to reproduce the results\n",
        "- All of the recipes are stored in `egs/<corpus>/<type>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cq6PbbfOCGM",
        "colab_type": "text"
      },
      "source": [
        "Supported corpus:\n",
        "- [CMUArctic database](http://www.festvox.org/cmu_arctic/): `egs/arctic`, 16 kHz, English, Several speakers.\n",
        "- [LJ Speech database](https://keithito.com/LJ-Speech-Dataset/): `egs/ljspeech` 22.05 kHz, English, Single female speaker.\n",
        "- [M-AILABS speech database](http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/):`egs/m-ailabs-speech`: 16 kHz, various speakers\n",
        "\n",
        "About supported type, see detail in https://github.com/kan-bayashi/PytorchWaveNetVocoder/tree/master/egs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97A3E2pWOCGN",
        "colab_type": "text"
      },
      "source": [
        "## Run the demo recipe\n",
        "\n",
        "Let us run the demo recipe `egs/arctic/sd-mini`.\n",
        "\n",
        "- Small version of `egs/arctic/sd`\n",
        "- Use subset of all of the utterances\n",
        "- **Cannot build a good model** but the flow is **the same**\n",
        "\n",
        "You can understand each stage within 30 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "8"
        },
        "id": "oQ5dDwn9OCGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move on the recipe directory\n",
        "import os\n",
        "os.chdir(\"./PytorchWaveNetVocoder/egs/arctic/sd-mini\")\n",
        "!echo $(pwd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0hduYVROCGQ",
        "colab_type": "text"
      },
      "source": [
        "Files in the recipe are as follows:\n",
        "- `conf`: Directory including config files\n",
        "- `path.sh`: Script to set the environmental variables.\n",
        "- `run.sh`: Main script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "b5vnaoZlOCGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree -L 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwHV5CBAOCGS",
        "colab_type": "text"
      },
      "source": [
        "`conf` includes f0 setting files whose name format is `<speaker_name>.f0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "13"
        },
        "id": "BN9oygelOCGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikvvc-LuOCGW",
        "colab_type": "text"
      },
      "source": [
        "`<speaker_name>.f0` includes `min_f0 max_f0`.  \n",
        "These values are predecided by ourselve, so you can modify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhwn84hyOCGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat conf/slt.f0  # (minf0 maxf0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8u4S9wnOCGZ",
        "colab_type": "text"
      },
      "source": [
        "All of the hyperparameters are written in `run.sh`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YUoi3kuYOCGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 69 run.sh "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdY3LbVxOCGc",
        "colab_type": "text"
      },
      "source": [
        "Let us introduce these parameters in detail later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "4"
        },
        "id": "A9t8l_XSOCGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional) here you can add your command to check the file!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjCVO3Z5OCGf",
        "colab_type": "text"
      },
      "source": [
        "### Overview of the recipe\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/overview.png?raw=1 width=80%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYwSb2cOCGf",
        "colab_type": "text"
      },
      "source": [
        "If run `run.sh`, all of stages will be performed.\n",
        "\n",
        "But we can specify the stage to run with `--stage` options.\n",
        "\n",
        "- `run.sh --stage 0`: Run only the stage 0\n",
        "- `run.sh --stage 012`: Run the stages 0, 1, and 2.\n",
        "\n",
        "Here, let us run each stage step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f2usBDtOCGg",
        "colab_type": "text"
      },
      "source": [
        "### Stage 0: Data preparation\n",
        "\n",
        "This stage performs download of corpus and list preparation.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_0.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ttXn9WhOCGg",
        "colab_type": "text"
      },
      "source": [
        "In arctic, there are seven speakers.  \n",
        "Here let us use `slt` to build a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "scrolled": false,
        "id": "TvLXGYbHOCGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you can specify the speaker via --spk (default=slt)\n",
        "!./run.sh --stage 0 --spk slt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPuA0MsAOCGj",
        "colab_type": "text"
      },
      "source": [
        "Corpus is saved in\n",
        "- `downloads/cmu_us_<spk_name>_arctic_mini`\n",
        "\n",
        "Two lists of wav files are created.\n",
        "- `data/tr_slt/wav.scp`: wav list file for training\n",
        "- `data/ev_slt/wav.scp`: wav list file for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "id": "JwQ8B4QVOCGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree -L 3 -I local"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfByAVMgOCGn",
        "colab_type": "text"
      },
      "source": [
        "The list file is that:\n",
        "- Each line has the path of wav file\n",
        "- All of the lines are sorted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "13"
        },
        "id": "exQT922fOCGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " !head -n 3 data/*_slt/wav.scp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIOVYZ3jOCGp",
        "colab_type": "text"
      },
      "source": [
        "Here we use 32 utts for training, 4 for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCIKWlISOCGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wc -l < data/tr_slt/wav.scp\n",
        "!wc -l < data/ev_slt/wav.scp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZdDhqp1OCGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBfJ946AOCGw",
        "colab_type": "text"
      },
      "source": [
        "### Stage 1: Feature extraction\n",
        "\n",
        "This stage performs feature extraction with the\n",
        "list file.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_1.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "z_PGg2ByOCGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters related to stage 1\n",
        "!head -n 36 run.sh | tail -n 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "scrolled": false,
        "id": "jWfSr_7mOCGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run stage 1 with default settings\n",
        "!./run.sh --stage 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huar_KV-OCG2",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters can be changed via command line.  \n",
        "But it will overwrite the existing ones. Be careful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "s-FibowoOCG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example of changing hyperparameters of feature extraction\n",
        "# !./run.sh --stage 1 --mcep_dim 30 --shiftms 10 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kii4bf_aOCG6",
        "colab_type": "text"
      },
      "source": [
        "Extracted features are saved as `hdf5` in\n",
        "- `hdf5/tr_slt/*.h5`: Feature file of training data \n",
        "- `hdf5/ev_slt/*.h5`: Feature file of evaluation data\n",
        "\n",
        "Lists of feature files are created \n",
        "- `data/tr_slt/feats.scp`\n",
        "- `data/ev_slt/feats.scp`\n",
        "\n",
        "High pass filtered training wav files are saved in\n",
        "- `wav_hpf/tr_slt/*.wav`: Filtered wav file of training data\n",
        "\n",
        "List of filetered wav files is created\n",
        "- `data/tr_slt/wav_hpf.scp`: List of filtered wav files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "15"
        },
        "scrolled": false,
        "id": "qNF2NT7aOCG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree -L 3 -I \"*.f0|local|cmu_*\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyRjmkj_OCG8",
        "colab_type": "text"
      },
      "source": [
        "Let us check the list file format:\n",
        "- Each line has the path of feature or wav\n",
        "file  \n",
        "- All of the lines are sorted\n",
        "- Assume that all of the lists has the same\n",
        "order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "16"
        },
        "id": "xZMw8EdJOCG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 3 data/*_slt/feats.scp\n",
        "!echo \"\"\n",
        "!head -n 3 data/tr_slt/wav_hpf.scp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQQHUc8AOCG_",
        "colab_type": "text"
      },
      "source": [
        "hdf5 format can be loaded as `numpy.ndarray` in python using `h5py` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "17"
        },
        "id": "UvB9QshbOCG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "with h5py.File(\"hdf5/tr_slt/arctic_a0001.h5\") as f:\n",
        "    print(f.keys())\n",
        "    feat = f[\"world\"][()]\n",
        "# or you can use our utils\n",
        "from wavenet_vocoder.utils import read_hdf5\n",
        "feat = read_hdf5(\"hdf5/tr_slt/arctic_a0001.h5\", \"world\")\n",
        "print(\"Feature shape: (#num_frames=%d, #feature_dims=%d)\" % (feat.shape[0], feat.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clM69wDKOCHB",
        "colab_type": "text"
      },
      "source": [
        "The feature is extracted with World.\n",
        "- `U/V binary` (1 dim)\n",
        "- `continuous F0` (1 dim), \n",
        "- `mcep`(25 dim = `mcep_dim + 1`) \n",
        "- `ap` (1 dim)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "18"
        },
        "id": "NH714CQ9OCHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(feat[:, 0])\n",
        "plt.title(\"U/V binary\")\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(feat[:, 1])\n",
        "plt.title(\"Continuous F0\")\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.imshow(feat[:, 2:26].T, aspect=\"auto\")\n",
        "plt.title(\"Mel-cepstrum\")\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(feat[:, -1])\n",
        "plt.title(\"Aperiodicity\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx5NPcq5OCHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKbrLs97OCHL",
        "colab_type": "text"
      },
      "source": [
        "### Stage 2: Statistics calculation\n",
        "\n",
        "This stage calculates the mean and variance of features.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_2.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "19"
        },
        "scrolled": false,
        "id": "FXKTdnsGOCHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run stage 2 with default settings\n",
        "!./run.sh --stage 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7D-17nTOCHP",
        "colab_type": "text"
      },
      "source": [
        "Calculated statistics are saved as `hdf5` format in\n",
        "- `data/tr_slt/stats.h5`\n",
        "\n",
        "`stats.h5` is used for:\n",
        "- Feature normalization during training\n",
        "- Calculation of noise shaping filter coefficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "20"
        },
        "id": "rUy2vLMcOCHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|local|cmu_*\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz3WrzeiOCHS",
        "colab_type": "text"
      },
      "source": [
        "`stats.h5` can be loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "21"
        },
        "id": "sKx0hJBzOCHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
        "    print(f.keys())\n",
        "    print(f['world'].keys())\n",
        "    mean = f['world']['mean'][()]\n",
        "    scale = f['world']['scale'][()]\n",
        "    print(mean.shape)\n",
        "    print(scale.shape)\n",
        "    \n",
        "# or you use our utils\n",
        "mean = read_hdf5(\"data/tr_slt/stats.h5\", \"world/mean\")\n",
        "scale = read_hdf5(\"data/tr_slt/stats.h5\", \"world/scale\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66JBn3w1OCHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here you can check the file with your commands!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBHugRLpOCHc",
        "colab_type": "text"
      },
      "source": [
        "### Stage 3: Noise weighting\n",
        "\n",
        "This stage applies noise weighting filter to training\n",
        "wav files.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_3.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWrS7_1gOCHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters related to stage 3\n",
        "!head -n 38 run.sh | tail -n 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "22"
        },
        "id": "2wh3EAQwOCHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run stage 3 with default settings\n",
        "!./run.sh --stage 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-zzPcqcOCHg",
        "colab_type": "text"
      },
      "source": [
        "If `use_noise_shaping=false`, `stage 3` will be skipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUDnKoH7OCHh",
        "colab_type": "text"
      },
      "source": [
        "Noise weighting filtered wav files are saved in\n",
        "- `wav_nwf/tr_slt/*.wav`\n",
        "\n",
        "The list of noise weighting filtered wav files is saved as\n",
        "- `data/tr_slt/wav_nwf.scp`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "23"
        },
        "id": "pYirsp-_OCHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree -L 3 -I \"*.f0|*[0-9].h5|local|cmu_*\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mkOZLFMOCHk",
        "colab_type": "text"
      },
      "source": [
        "Let us check the difference of waveform here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "25"
        },
        "id": "aiqCZas7OCHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# listen to the samples\n",
        "import IPython.display\n",
        "IPython.display.display(IPython.display.Audio(\"wav_hpf/tr_slt/arctic_a0001.wav\"))\n",
        "IPython.display.display(IPython.display.Audio(\"wav_nwf/tr_slt/arctic_a0001.wav\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "26"
        },
        "id": "fJNnnzcGOCHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show spectrogram\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "x, fs = sf.read(\"wav_hpf/tr_slt/arctic_a0001.wav\")\n",
        "x_ns, fs = sf.read(\"wav_nwf/tr_slt/arctic_a0001.wav\")\n",
        "plt.figure(figsize=(16, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.specgram(x, Fs=fs)\n",
        "plt.title(\"Original spectrogram\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.specgram(x_ns, Fs=fs)\n",
        "plt.title(\"Noise weighting filtered spectrogram\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_xjwxAUOCHo",
        "colab_type": "text"
      },
      "source": [
        "Filtering related parameters `mlas/coef` and `mlsa/alpha` are added in `data/tr_slt/stats.h5`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "27"
        },
        "id": "QvkrKr_6OCHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
        "    print(f.keys())\n",
        "    print(f[\"mlsa\"].keys())\n",
        "    print(f[\"mlsa\"][\"alpha\"])\n",
        "    print(f[\"mlsa\"][\"coef\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp21NACFOCHt",
        "colab_type": "text"
      },
      "source": [
        "`mlsa/coef` is the coefficient of MLSA filter, which is calculated from averaged mel-cepstrum and `mag`.  \n",
        "`mlsa/alpha` is the hyperparameter `alpha`, all pass filter coefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfHNQFIrOCHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shex6xJQOCHv",
        "colab_type": "text"
      },
      "source": [
        "### Stage 4: WaveNet training\n",
        "\n",
        "This stage trains WaveNet using extracted\n",
        "features and noise weighting filtered wav files.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_4.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5xzseEFOCHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters related to stage 4\n",
        "!head -n 59 run.sh | tail -n 19"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "28"
        },
        "scrolled": false,
        "id": "83C7uI-lOCHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run stage 4 with default settings\n",
        "!./run.sh --stage 4 --iters 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gjJK5JQOCH2",
        "colab_type": "text"
      },
      "source": [
        "Default network structure in `egs/arctic/sd-mini`.\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/wavenet.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhrGmcGOOCH2",
        "colab_type": "text"
      },
      "source": [
        "Example when `dilation_depth=3` and `dilation_repeat=2`.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/structure_ex.png?raw=1 width=45%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aangSJTOCH3",
        "colab_type": "text"
      },
      "source": [
        "Make a batch by split a waveform into pieces.\n",
        "- `batch_size`: Number of batches\n",
        "- `batch_length`: Length of each batch\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/batch.png?raw=1 width=65%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTZalPXOCH3",
        "colab_type": "text"
      },
      "source": [
        "Model parameters are saved as  \n",
        "- `exp/tr_arctic_16k_sd_world_slt_*/checkpoint-*.pkl` \n",
        "\n",
        "Modle configuration is saved as  \n",
        "- `exp/tr_arctic_16k_sd_world_slt_*/model.conf`\n",
        "\n",
        "The directory name is automatically set to be unique depending on hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "29"
        },
        "id": "NQSsWrtZOCH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|*.scp|*.log|local|cmu_*\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf0e7GdkOCH-",
        "colab_type": "text"
      },
      "source": [
        "Model configuration file can be loaded as `argparse.Namespace`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "30"
        },
        "id": "HF_QoqaIOCH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "conf = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf\")\n",
        "print(conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD5uE2O2OCID",
        "colab_type": "text"
      },
      "source": [
        "Model parameters `checkpoint-*.pkl` can be loaded as `dict` which contains\n",
        "following information:\n",
        "- `iterations`: Number of iterations of this parameters\n",
        "- `optimizer`: `Dict` of states of optimizer\n",
        "- `model`: `OrderedDict` of Model\n",
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "31"
        },
        "id": "M33QSDOiOCIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_dict = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl\")\n",
        "print(state_dict.keys())\n",
        "print(state_dict[\"iterations\"])\n",
        "print(state_dict[\"optimizer\"].keys())\n",
        "print(state_dict[\"model\"].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hVUaum5OCIG",
        "colab_type": "text"
      },
      "source": [
        "You can resume training from `checkpoint-*.pkl` file with `--resume` options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "32"
        },
        "scrolled": false,
        "id": "d_xFq7PzOCIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./run.sh --stage 4 \\\n",
        "    --iters 1000 \\\n",
        "    --resume exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iptBsnc3OCIJ",
        "colab_type": "text"
      },
      "source": [
        "You can train using multi-gpu with `--n_gpus` option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_QbyAGOCIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In colab, we can use only a single gpu :(\n",
        "# batch_size must be >= n_gpus\n",
        "# !./run.sh --stage 4 --n_gpus 2 --batch_size 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqJu8Is9OCIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here you can check the file with your commands!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYaU7sn9OCIQ",
        "colab_type": "text"
      },
      "source": [
        "### Stage 5: WaveNet decoding\n",
        "\n",
        "This stage performs decoding of evaluation data.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_5.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0BF2sW0OCIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters related to stage 5\n",
        "!head -n 69 run.sh | tail -n 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "35"
        },
        "id": "4pN7o8OUOCIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run stage 5 with default setting\n",
        "!./run.sh --stage 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW7_QpGxOCIX",
        "colab_type": "text"
      },
      "source": [
        "You can specify the `checkpoint-*.pkl` file used for decoding and directory to\n",
        "be saved via `--checkpoint` and `--outdir` options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU8vPco4OCIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it takes times, comment out\n",
        "# !./run.sh --stage 5 \\\n",
        "#     --checkpoint exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-100.pkl\n",
        "#     --outdir exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/wav_ckpt_100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_eN-KZQOCIY",
        "colab_type": "text"
      },
      "source": [
        "We can use multi-gpu decoding via `--n_gpus` option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjJAjW94OCIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In colab, we can use only a single gpu :(\n",
        "# !./run.sh --stage 5 --n_gpus 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzjl6WUJOCIe",
        "colab_type": "text"
      },
      "source": [
        "Generated wav files are saved in \n",
        "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "36"
        },
        "id": "LVU3duzAOCIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IBcmGRLOCIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVRnGHNwOCIm",
        "colab_type": "text"
      },
      "source": [
        "### Stage 6: Noise shaping\n",
        "\n",
        "This stage applies noise shaping filter to generated wav files.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_6.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "37"
        },
        "id": "-gTal-tjOCIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run stage 6 with default setting\n",
        "!./run.sh --stage 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD4-2wI4OCIq",
        "colab_type": "text"
      },
      "source": [
        "Restored wav files are saved in\n",
        "\n",
        "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav_nsf`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "38"
        },
        "scrolled": false,
        "id": "eFXLEouIOCIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l3mM-XwOCIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_7w2_WpOCIx",
        "colab_type": "text"
      },
      "source": [
        "Finished! Unfortunately, generated samples are just-like a noise.  \n",
        "So Let us check the samples which trained with `egs/arctic/sd` from  \n",
        "https://kan-bayashi.github.io/WaveNetVocoderSamples/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iluR3doLOCIx",
        "colab_type": "text"
      },
      "source": [
        "## Use pretrained model as vocoder\n",
        "\n",
        "Here we show how-to-use pretrained model as\n",
        "vocoder.  \n",
        "What we need to prepare is following three files:\n",
        "\n",
        "- `model.conf`:\n",
        "Model configuration file.\n",
        "- `checkpoint-*.pkl`: Model parameter file.\n",
        "- `stats.h5`: Statistics file.\n",
        "\n",
        "Let us pack following files into\n",
        "`pretrained_model/` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "39"
        },
        "id": "k0Bx_aP-OCIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize trained model in the directory\n",
        "!mkdir pretrained_model\n",
        "!cp -v exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/stats.h5 \\\n",
        "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf \\\n",
        "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-1000.pkl \\\n",
        "    pretrained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJOibWZMOCI0",
        "colab_type": "text"
      },
      "source": [
        "First, please prepare the list file of feature files to be decoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "40"
        },
        "id": "55RPhY4eOCI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# here make a dummy features and the stored as hdf5 with key \"/world\"\n",
        "os.makedirs(\"dummy\", exist_ok=True)\n",
        "for idx, n_frames in enumerate([10, 20, 30, 40]): \n",
        "    x = np.random.randn(n_frames, 28)  # (#num_frames, #feature_dims)\n",
        "    with h5py.File(\"dummy/dummy_%d.h5\" % idx, \"w\") as f:\n",
        "        f[\"world\"] = x\n",
        "\n",
        "# make a list of features to be decoded.\n",
        "!find dummy -name \"*.h5\" > dummy_feats.scp\n",
        "\n",
        "# check\n",
        "!cat dummy_feats.scp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFISvNbQOCI2",
        "colab_type": "text"
      },
      "source": [
        "Run the `--stage 56` by specifying `--feats` in the recipe directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "41"
        },
        "id": "-YX5edBSOCI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decode with pretrained model through the recipe\n",
        "!./run.sh --stage 56 \\\n",
        "    --outdir dummy_feats_wav \\\n",
        "    --feats dummy_feats.scp \\\n",
        "    --checkpoint pretrained_model/checkpoint-1000.pkl\n",
        "!ls dummy_feats_wav*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p29lJuS_OCI4",
        "colab_type": "text"
      },
      "source": [
        "If you want to use outside of the recipe, directly call python scripts stored in\n",
        "`wavenet_vocoder/bin`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "42"
        },
        "id": "rzZbC1snOCI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decode with pretrained model\n",
        "!python ../../../wavenet_vocoder/bin/decode.py \\\n",
        "     --feats dummy_feats.scp \\\n",
        "     --outdir dummy_feats_wav_2 \\\n",
        "     --checkpoint pretrained_model/checkpoint-1000.pkl \\\n",
        "     --fs 16000 \\\n",
        "     --n_gpus 1 \\\n",
        "     --batch_size 4\n",
        "# make list of wav files to be filtered\n",
        "!find dummy_feats_wav_2 -name \"*.wav\" > dummy_feats_wav_2/wav.scp\n",
        "# apply noise shaping filter\n",
        "!python ../../../wavenet_vocoder/bin/noise_shaping.py \\\n",
        "     --waveforms dummy_feats_wav_2/wav.scp \\\n",
        "     --outdir dummy_feats_wav_2_nsf \\\n",
        "     --stats pretrained_model/stats.h5 \\\n",
        "     --fs 16000 \\\n",
        "     --shiftms 5\n",
        "!ls dummy_feats_wav_2*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvS6XRQOCI8",
        "colab_type": "text"
      },
      "source": [
        "## Combine with Sprocket\n",
        "\n",
        "Let us show how-to-combine wavenet vocoder with voice conversion toolkit [sprocket](https://github.com/k2kobayashi/sprocket).    \n",
        "Here, we generate converted voice with pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XESIgdXKOCI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# changed directory\n",
        "!mkdir ../../../../conversion_example\n",
        "os.chdir(\"../../../../conversion_example\")\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31HLVXa9OCI-",
        "colab_type": "text"
      },
      "source": [
        "First, download pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnGWOPXuOCI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download sprocket model\n",
        "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
        "    \"https://drive.google.com/open?id=1PiGDyYDQt0b4h6KAV1MOmDxHjHUv1cT6\" \\\n",
        "    downloads/sprocket_pretrained\n",
        "\n",
        "# download wavenet vocoder model\n",
        "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
        "    \"https://drive.google.com/open?id=1AhtRB0vTkjDrum-dfgaiXnQgsAAiYMGW\" \\\n",
        "    downloads/wavenet_vocoder_pretrained\n",
        "\n",
        "# download wav samples\n",
        "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
        "    \"https://drive.google.com/open?id=1kBwF7ejyCR5aI9FitmMSCnWdPCNVouqg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7jWbJXaOCJA",
        "colab_type": "text"
      },
      "source": [
        "- Sprocket pretrained model\n",
        "    - `GMM_mcep.pkl`: GMM param file for mcep conversion.\n",
        "    - `<src_spk>.yml`: Source speaker yaml file.\n",
        "    - `<src_spk>-<tar_spk>.yml`: Source-target speaker pair yaml file.\n",
        "    - `<src_spk>.h5`: Statistics file of source speaker.\n",
        "    - `<tar_spk>.h5`: Statistics file of target speaker.\n",
        "    - `cvgv.h5`: Statistics file of global variance for converted features.\n",
        "    \n",
        "- Target speaker WaveNet vocoder pretrained model\n",
        "    - `model.conf`: Model configuration file.\n",
        "    - `checkpoint-*.pkl`: Model parameter file.\n",
        "    - `stats.h5`: Statistics file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qI9rPfOCJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls downloads/*pretrained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wfVejfzOCJD",
        "colab_type": "text"
      },
      "source": [
        "Next, extract features and then convert them to target speaker's one.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46STdVHDOCJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "![ ! -e hdf5 ] && mkdir hdf5\n",
        "![ ! -e wav ] && mkdir wav\n",
        "!PYTHONPATH=../sprocket/example/src \\\n",
        "    python ../sprocket/sprocket/bin/convert_feats.py \\\n",
        "        --cvmcep0th True \\\n",
        "        --cvcodeap True \\\n",
        "        --cvgvstats downloads/sprocket_pretrained/cvgv.h5 \\\n",
        "        --org_yml downloads/sprocket_pretrained/rms.yml \\\n",
        "        --pair_yml downloads/sprocket_pretrained/rms-slt.yml \\\n",
        "        --org_stats downloads/sprocket_pretrained/rms.h5 \\\n",
        "        --tar_stats downloads/sprocket_pretrained/slt.h5 \\\n",
        "        --mcepgmmf downloads/sprocket_pretrained/GMM_mcep.pkl \\\n",
        "        --iwav downloads/samples/src/arctic_b0536.wav \\\n",
        "        --cvfeats hdf5/arctic_b0536.h5 \\\n",
        "        --owav wav/arctic_b0536.wav\n",
        "!ls hdf5 wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tusbI4gOCJF",
        "colab_type": "text"
      },
      "source": [
        "Then generate waveform with pretrained wavenet using converted features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxie4T6aOCJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: require too much time.\n",
        "# decode with wavenet vocoder\n",
        "!find hdf5 -name \"*.h5\" > hdf5/feats.scp\n",
        "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/decode.py \\\n",
        "     --feats hdf5/feats.scp \\\n",
        "     --outdir wav_wnv \\\n",
        "     --checkpoint downloads/wavenet_vocoder_pretrained/checkpoint-final.pkl \\\n",
        "     --fs 16000 \\\n",
        "     --n_gpus 1 \\\n",
        "     --batch_size 4\n",
        "# apply noise shaping filter\n",
        "!find wav_wnv -name \"*.wav\" > wav_wnv/wav.scp\n",
        "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/noise_shaping.py \\\n",
        "     --waveforms wav_wnv/wav.scp \\\n",
        "     --outdir wav_wnv_nsf \\\n",
        "     --stats downloads/wavenet_vocoder_pretrained/stats.h5 \\\n",
        "     --fs 16000 \\\n",
        "     --shiftms 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "y6zVMilHOCJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# listen to pre-synthesized ones\n",
        "import IPython.display\n",
        "print(\"Source\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/src/arctic_b0536.wav\"))\n",
        "print(\"Target\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/tar/arctic_b0536.wav\"))\n",
        "print(\"Converted voice with vocoder\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/vocoder/arctic_b0536.wav\"))\n",
        "print(\"Converted voice with wavenet vocoder\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/wavenet_vocoder/arctic_b0536.wav\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWb19PmaOCJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"running time = %s minite\" % ((time.time() - start_time) / 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hsdbtwROCJU",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "- Introduced voice conversion with direct waveform modeling\n",
        "- Introduced Sprocket /  PytorchWaveNetVocoder\n",
        "    - Can build GMM-based VC / DIFFVC  & WaveNet vocoder\n",
        "    - Can combine both module to generate high quality converted voices\n",
        "\n",
        "Thank you for your attendance!  \n",
        "If you have time, please send us feedback via [Google form](https://forms.gle/28QrvGRBAAiKpWas8). "
      ]
    }
  ]
}